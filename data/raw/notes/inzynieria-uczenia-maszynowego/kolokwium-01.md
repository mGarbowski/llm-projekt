1. Co jest kluczowym elementem realizacji projektu stosującego UM? 
2. Jeśli mamy zbiór danych ustrukturyzowany tak, że wydaje się nadawać do trenowania modeli uczenia maszynowego, to czy warto “tracić czas” na poznawanie tych danych? 
3. Po co nam ekspert domenowy podczas realizacji projektów komercyjnych? 
4. Czy wszystkie iteracje procesu CRISP-DM wyglądają tak samo?

5. Jeśli w projekcie mamy do rozwiązania więcej niż jedno zadanie modelowania, to czy analizujemy jakość każdego modelu z osobna, czy raczej rozwiązania całościowego? 
6. Czy miary takie jak precyzja (precision) czy czułość (recall) mogą posłużyć do zdefiniowania biznesowych kryteriów sukcesu? 
7. Załóżmy, że mamy przeprowadzić segmentację klientów pod kątem ich miesięcznych wydatków w sklepie internetowym – jakie zadanie/zadania modelowania mogą tutaj być odpowiednie? 
8. Kiedy lepiej pomyśleć o wymaganiach niefunkcjonalnych – przed czy po zakończeniu modelowania?

9. Czy zawsze zmniejszenie obciążenia modelu jest korzystne? 
10. Jeżeli chcemy przeprowadzić strojenie hiperparametrów modelu, to jak będzie wyglądała procedura postępowania? Do czego będziemy stosować zbiór uczący, deweloperski i testowy? 
11. Jakie mogą być przyczyny tego, że rozkład danych “na produkcji” jest inny niż ten ze zbioru dostępnego do trenowania? 
12. Czy dokonując transferu wiedzy musimy “zamrozić” parametry oryginalnego modelu, czy możemy je “dotrenować” na nowych danych?

13. Czy problem niezbalansowania może występować w przypadku zadań regresji? 
14. Czy mając atrybut z brakami MNAR, da się uzyskać sytuację, w której braki będą MAR? 
15. Jakie są przykładowe metody uzupełniania braków w atrybutach dyskretnych? 
16. Czy każde zdarzenie o dużym wpływie na rynek akcji to tzw. czarny łabędź? 
17. Jaki rozkład p-stwa opisuje wynik rzutu n-ścienną kostką symetryczną? 
18. Jakim typem atrybutu jest dzień tygodnia?

19. Czy feature hashing można zastosować w przypadku zmiennych ciągłych? 
20. Dlaczego korelacja Pearsona nie nadaje się dla zmiennych dyskretnych? 
21. Skoro selekcja atrybutów zmniejsza liczbę sygnałów docierających do modelu – po co ją wykonywać? 
22. Czy do generowania interakcji możemy stosować więcej niż dwa atrybuty?

23. Czy strojenie hiperparametrów dobrze przeprowadzać z użyciem zbioru testowego? 
24. W jakiej sytuacji ta sama miara jakości może służyć do oceny modelu z perspektywy analitycznej i biznesowej? 
25. Jak zdefiniować hipersiatkę dla parametru, który może przyjmować wartości od 0.001, do 10 000? 
26. Jaką rolę pełni “kotwica” w metric learning?