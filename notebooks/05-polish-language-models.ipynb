{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a8d1490ac16b3b",
   "metadata": {},
   "source": [
    "# Użycie modeli językowych przystosowanych do języka polskiego\n",
    "\n",
    "Zmiana koncepcji - będę używać modeli, które testowałem na laboratorium\n",
    "Muszę zweryfikować jakie okno kontekstowe mają te modele, żeby odpowiednio dobrać długość chunków\n",
    "\n",
    "Na razie myślę że użyję prostego podziełu na chunki o stałej liczbie znaków\n",
    "\n",
    "Będę tworzyć jeden podział dokumentów więc chunki muszą się zmieścić do każdego modelu\n",
    "policze średnią liczbę tokenów na znak i zostawię bezpieczny zapas\n",
    "\n",
    "\n",
    "## Reranker\n",
    "Zgodnie z dokumentacją `polish-reranker-roberta-v3` wspiera kontekst 8192 tokenów\n",
    "\n",
    "## Retrieval\n",
    "\n",
    "### BM25\n",
    "Nie ma ograniczenia długości kontekstu\n",
    "\n",
    "### `mmlw-retrieval-roberta-base`\n",
    "\n",
    "Okno kontekstu 512 tokenów\n",
    "\n",
    "Dla przykładowego dokumentu 0.24 tokena na znak, 1.71 tokena na słowo\n",
    "\n",
    "## Genrator\n",
    "\n",
    "### Bielik\n",
    "W karcie modelu jest podany ogromny rozmiar kontekstu - nie powinno być problemu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be3c09a7f3bd9a",
   "metadata": {},
   "source": "## Przykładowy dokument"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d026d57e047fa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T11:10:59.835466782Z",
     "start_time": "2026-01-03T11:10:59.757015149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Definiowanie zadań modelowania\\n\\n## Kluczowe elementy\\n* Jakie jest zadanie biznesowe / domenowe\\n\\t* czy je dostatecznie dobrze rozumiemy\\n\\t* jakie są ograniczenia (czasowe, zasobów, formalne, ...)\\n* Jakie są zadania modelowania\\n\\t* czy są odpowiednie dane do budowy modeli\\n\\t* czy rozumiemy proces, który je wygenerował\\n* Jakie są kryteria sukcesu\\n\\t* na poziomie zadań modelowania (analityczne)\\n\\t* biznesowe\\n\\n## Formułowanie zadania biznesowego\\n* Potrzebny jest ekspert domenowy\\n\\t* np. product owner w metodologiach zwinnych\\n\\t* dobrze rozumie zagadnienie, którym mamy się zająć i wynikające z niego potrzeby\\n\\t* zna specyfikę pracy nad modelowaniem danych\\n\\t* jest osobą decyzyjną w zakresie projektu - wspólne ustalenia są wiążące\\n* Analizujemy kontekst w jakim występuje potrzeba biznesowa\\n\\t* jaka jest obecna sytuacja\\n\\t* co ma zostać\\xa0wprowadzone / zmienione\\n\\t* jakie właściwości / cechy powinno mieć docelowe rozwiązanie\\n\\t* jakie są założenia, oczekiwania, ograniczenia, zasoby\\n\\n## Biznesowe kryteria sukcesu\\n* Pytanie pomocnicze - co chcemy zrobić?\\n* Poprawiamy istniejące podejście?\\n\\t* niekoniecznie stosujące UM\\n* Spełniamy jasno sprecyzowane wymaganie klienta?\\n* Staramy się przewyższyć konkurencję / standardy branżowe?\\n* Prowadzimy wstępne prace badawcze / rozpoznajemy temat?\\n\\t* z ograniczeniami czasowymi / zasobowymi / budżetowymi\\n* Klienta interesuje zwrot z inwestycji?\\n\\n## Mapowanie zadania domenowego na analityczne\\n* Precyzujemy zadania modelowania i relacje między nimi\\n* Zazwyczaj - definiujemy jedno lub kilka zadań, których wyniki łączymy tak, aby realizowały zadanie biznesowe\\n* Określamy jak będziemy weryfikować kryteria sukcesu\\n\\t* pozwolą stwierdzić czy przygotowane rozwiązanie spełnia oczekiwania klienta\\n\\t* pomagają ustalić, czy potrzebna będzie jeszcze kolejna iteracja pracy nad projektem\\n\\nJeśli nie wiemy jak dany problem biznesowy w bezpośredni sposób wyrazić w postaci zadań modelowania - często skuteczne będzie jedno z dwóch podejść\\n\\t\\n* Wyszukiwanie analogii\\n\\t* staramy się zidentyfikować zadania domenowe o podobnej strukturze, dla których wiemy jak zdefiniować zadania analityczne\\n\\t* być może z innych dziedzin\\n* Dekompozycja\\n\\t* rozkładamy zadanie na bardziej podstawowe elementy, które wiemy jak rozwiązać\\n\\n## Podstawowe typy zadań modelowania\\n\\n### Klasyfikacja\\n* Przewidujemy klasę dla danych wejściowych\\n* Model $f: X \\\\rightarrow \\\\{c_1, c_2, \\\\ldots, c_m\\\\}$\\n* Przykładowe zastosowania\\n\\t* rozpoznawanie obrazów\\n\\t* detekcja spamu\\n\\t* określanie gatunku utworów muzycznych\\n* Przykładowe metody\\n\\t* regresja logistyczna\\n\\t* naiwny klasyfikator bayesowski\\n\\t* SVM\\n\\t* perceptrony wielowarstwowe (MLP)\\n\\n### Regresja / aproksymacja\\n* Predykcje są liczbami ciągłymi\\n* Model $f: X \\\\rightarrow Y \\\\subset \\\\mathbb{R}^m$\\n* Przykładowe zastosowania\\n\\t* przewidywanie parametrów procesów technologicznych, zysku z inwestycji, obrotów firmy\\n* Przykładowe metody\\n\\t* regresja liniowa\\n\\t* SVR\\n\\t* random forest\\n\\t* sieci MLP\\n\\n### Grupowanie\\n* Poszukujemy funkcji, która podzieli dane na rozłączne grupy\\n* Zadanie bez nadzoru\\n* Model $f: X \\\\rightarrow \\\\{g_1, \\\\ldots, g_m\\\\}$\\n\\t* liczba grup często ustalana empirycznie lub na podstawie wiedzy eksperckiej\\n* Przykładowe zastosowania\\n\\t* segmentacja klientów\\n\\t* automatyczna kategoryzacja produktów\\n\\t* wyszukiwanie podobnych produktów\\n* Przykładowe metody\\n\\t* k-średnich\\n\\t* sieci SOM\\n\\t* affinity propagation\\n\\t* DBSCAM\\n\\n### Analiza szeregów czasowych\\n* Chcemy przewidzieć przyszłe wartości szeregu czasowego $x_1, x_2, \\\\ldots$\\n\\t* $x_i \\\\in \\\\mathbb{R}$\\n* Postać modelu zależy od konkretnej metody\\n* Przykładowe zastosowania\\n\\t* przewidywanie wahań giełdowych\\n\\t* wskaźników ekonometrycznych\\n\\t* temperatury powietrza\\n* Przykładowe metody\\n\\t* ARIMA\\n\\t* wygładzanie wykładnicze\\n\\t* metody regresji\\n\\n### Modelowanie sekwencji\\n* Poszukujemy funkcji, która dla danej sekwencji wejściowej będzie przewidywać sekwencję / sygnał wyjściowy\\n* Model $f: X \\\\rightarrow Y$\\n\\t* $X$ przestrzeń sygnałów wejściowych\\n\\t* $Y$ przestrzeń sygnałów wyjściowych\\n* Przykładowe zastosowania\\n\\t* tłumaczenie maszynowe\\n\\t* klasyfikacja tekstów\\n* Przykładowe metody\\n\\t* sieci LSTM\\n\\t* GRU\\n\\t* transformery\\n\\n### Rankingowanie\\n* Poszukujemy funkcji porządkującej elementy z danej przestrzeni pod kątem ich relewantności dla użytkownika\\n\\t* np. produkty opisane przez zestaw atrybutów\\n* W przypadku podejścia oceniającego relewantność pojedynczego elementu\\n\\t* $f: X \\\\rightarrow [0,b]$\\n\\t* $b < \\\\infty$\\n* Przykładowe zastosowania\\n\\t* sortowanie wyników wyszukiwania\\n* Stosowane są metody z dziedziny learning-to-rank\\n\\n### Generowanie rekomendacji\\n* Poszukujemy funkcji, która na podstawie wektora informacji kontekstowych dla danego użytkownika zwróci wektor elementów dla tego użytkownika odpowiednich\\n* Przykładowe zastosowania\\n\\t* polecanie utworów muzycznych, filmów, produktów w sklepach internetowych\\n* Przykładowe metody\\n\\t* collaborative filtering\\n\\t* k-najbliższych sąsiadów\\n\\t* Gru4Rec\\n\\n### Analiza przeżycia\\n* Przewidywanie długości życia badanych obiektów w przypadku, gdy występuje tzw. cenzurowanie danych\\n\\t* nie mamy kompletu informacji o długości życia obiektów w danej próbie\\n* Przykładowe zastosowania\\n\\t* badania medyczne\\n\\t* diagnostyka silników lotniczych (predictive maintenance)\\n\\t* modelowanie aukcji vickreya\\n* Przykładowe metody\\n\\t* krzywe Kaplana-Meiera lub Wibulla\\n\\n### Modele generatywne\\n* Modele generatywne umożliwiają konstrukcję elementów należących do skomplikowanych przestrzeni (np. obrazów)\\n\\t* Generative Adversarial Networks\\n\\t* Variational Autoencoders\\n\\t* Stable Diffusion\\n\\t* LLMs\\n\\n### Modele językowe\\n* Pozwalają oszacować prawdopodobieństwo $P(w|w_1,\\\\ldots,w_n)$\\n\\t* $w$ - interesujące nas słowo / token\\n\\t* $w_1,\\\\ldots,w_n$ - słowa / tokeny zapewniające kontekst\\n* W zależności od tego jak wygląda kontekst możemy mieć modele do generowania tekstu lub jego rozumienia\\n\\t* Generowanie (np. GPT) - *Ala ma ???*\\n\\t* Rozumienie (np. BERT) - *Ala ??? kota*\\n\\n## Składniki budowy modelu predykcyjnego\\n* Struktura modelu\\n* Funkcja celu\\n* Metoda optymalizacji\\n\\n## Wstępny dobór metod modelowania\\n* Typ zadania\\n* Charakter danych uczących\\n\\t* wielkość zbioru\\n\\t* wartości brakujące\\n\\t* typy atrybutów\\n\\t* rozkłady atrybutów\\n\\t* struktura zależności między atrybutami\\n* Jakie modele sugeruje nam literatura? Jakie wcześniej nam zadziałały?\\n* Jakie metody dobrze znamy?\\n* Co jest wspierane przez ekosystem, w którym będzie działać model?\\n* Jakie mamy zasoby sprzętowe?\\n* Jak istotne będzie wyjaśnianie predykcji (i inne aspekty niefunkcjonalne)?\\n\\n### Podsumowanie\\n* Biznesowe kryteria / miary jakości\\n\\t* perspektywa klienta\\n\\t* sprawdzają czy rzeczywiste zadanie zostanie rozwiązane\\n* Zadania modelowania, analityczne kryteria sukcesu\\n\\t* ułatwiają pracę analitykom\\n\\t* pozwalają dopasować się do standardowych zagadnień\\n* Mapowanie czasami nie jest oczywiste, wymaga doświadczenia\\n* Dochodzi jeszcze perspektywa techniczna\\n* Nie zawsze da się zmierzyć kryteria biznesowe na danych offline\\n\\n## Wymagania\\n### Wymagania techniczne\\n* Zazwyczaj *typowe* dla systemów informatycznych\\n\\t* maksymalny czas odpowiedzi\\n\\t* minimalna przepustowość\\n\\t* ograniczenia na użycie zasobów\\n\\t* dopasowanie do konkretnych technologii\\n\\t* itd.\\n\\n### Wymagania funkcjonalne\\n* Określają cel, jaki ma realizować system - jego funkcje\\n* Powinny być kompletne i spójne\\n* W przypadku UM - specyfikowane przez miary jakości i kryteria sukcesu\\n\\n### Wymagania niefunkcjonalne\\n* Dotyczą tego jak ma działać\\n* Określają dodatkowe ograniczenia odnoście jego zachowania\\n\\n### Wymagania niefunkcjonalne w UM\\n* Zagadnienie jeszcze nie do końca zbadane\\n\\t* mało publikacji na ten temat\\n* Powoli krystalizuje się zestaw istotnych z praktycznego punktu widzenia aspektów\\n\\t* transparentność / zdolność wyjaśniania predykcji (XAI)\\n\\t* kwestie związane z bezpieczeństwem i prywatnością (adversarial examples, prywatność różnicowa)\\n\\t* fairness / kwestie dyskryminacji\\n\\t* testowalność, niezawodność, łatwość utrzymania, itd.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "example_doc_path = Path(\n",
    "    \"../data/raw/notes/inzynieria-uczenia-maszynowego/02-zadania-modelowania.md\"\n",
    ")\n",
    "example_doc = example_doc_path.read_text()\n",
    "example_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eb238810c8f22ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T11:11:27.591269466Z",
     "start_time": "2026-01-03T11:11:27.556035584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7882"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "931e376fdd3a7538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T11:15:46.719782993Z",
     "start_time": "2026-01-03T11:15:46.682711871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1090"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_example_doc_words = len(example_doc.split())\n",
    "n_example_doc_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bfe8ebf3dba433",
   "metadata": {},
   "source": "## bi-enkoder"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80f199d2446e88e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T11:09:09.392016992Z",
     "start_time": "2026-01-03T11:08:05.393637497Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e34ef5c8d6428c89000d0600d3210c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0096ad6691384c44880b494c2e4085e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/127 [00:00<?, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f487a83f60947e48b61be083366f16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1d3d2978f74d73bc8b4b0be32e83b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf0c95ac1fb4250a361a4ddcc450b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/862 [00:00<?, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d142d82734994450926d5d66e894786a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef448fad0bc4a08bcbf7c6ee0a8938d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ea3a0bec1e4ff0bc15832376b84063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9450d3eab5fe4d6c9e2cfc25a1f3a8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/75.0 [00:00<?, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414a2392966242eab4ba74d7706163a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdaecb5bc6e4d178f06ea5b4dc132f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "bi_encoder_model = SentenceTransformer(\"sdadas/mmlw-retrieval-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224d918a752fae4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T11:09:25.649136472Z",
     "start_time": "2026-01-03T11:09:25.611940270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_encoder_model.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdd8969891915f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T11:09:30.396260861Z",
     "start_time": "2026-01-03T11:09:30.350753715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='sdadas/mmlw-retrieval-roberta-base', vocab_size=50001, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50000: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_encoder_model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4b006296536b2d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T11:15:53.517404278Z",
     "start_time": "2026-01-03T11:15:53.477896365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnio tokenów na znak: 0.2359\n",
      "Średnio tokenów na słowo: 1.7055\n",
      "Liczba tokenów: 1859\n",
      "Przykładowe tokeny:\n",
      "▁\n",
      "#\n",
      "▁Defini\n",
      "owanie\n",
      "▁zadań\n",
      "▁model\n",
      "owania\n",
      "▁\n",
      "##\n",
      "▁Klucz\n",
      "owe\n",
      "▁elementy\n",
      "▁\n",
      "*\n",
      "▁Jakie\n",
      "▁jest\n",
      "▁zadanie\n",
      "▁biznesowe\n",
      "▁\n",
      "/\n"
     ]
    }
   ],
   "source": [
    "bi_encoder_tokens = bi_encoder_model.tokenizer.tokenize(example_doc)\n",
    "print(f\"Średnio tokenów na znak: {len(bi_encoder_tokens) / len(example_doc):.4f}\")\n",
    "print(f\"Średnio tokenów na słowo: {len(bi_encoder_tokens) / n_example_doc_words:.4f}\")\n",
    "print(f\"Liczba tokenów: {len(bi_encoder_tokens)}\")\n",
    "print(\"Przykładowe tokeny:\")\n",
    "for token in bi_encoder_tokens[:20]:\n",
    "    print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
